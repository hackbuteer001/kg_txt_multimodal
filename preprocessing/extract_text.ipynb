{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re, json, ast\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "\n",
    "ROOT_PATH = 'result'\n",
    "if not os.path.isdir(ROOT_PATH):\n",
    "    os.mkdir(ROOT_PATH)\n",
    "    \n",
    "\n",
    "def load_csv_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def save_csv_file(csv_data, file_path):\n",
    "    csv_data.to_csv(file_path, index=False)\n",
    "    return print('save successfully!')\n",
    "\n",
    "\n",
    "def load_noteevents(file_path):\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # dataframe dtype config\n",
    "    df.CHARTDATE = pd.to_datetime(df.CHARTDATE, format='%Y-%m-%d', errors='raise')\n",
    "    df.CHARTTIME = pd.to_datetime(df.CHARTTIME, format='%Y-%m-%d %H:%M:%S', errors='raise')\n",
    "    df.STORETIME = pd.to_datetime(df.STORETIME)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. extract sections\n",
    "\n",
    "**input:**\n",
    "- mimic_table/NOTEEVENTS.csv\n",
    "\n",
    "**output:**\n",
    "- result/NOTEEVENTS_SECTIONS.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Table --> Sections\n",
    "\n",
    "1. load NOTEEVENTS.csv\n",
    "\n",
    "2. get discharge sumamry notes\n",
    "    a) NOTEVENTS.CATEGORY = 'Discharge Summary'\n",
    "    b) NOTEVENTS.DESCRIPTION = 'Report'\n",
    "    c) eliminate a short-note\n",
    "\n",
    "3. preprocess discharge sumamry notes\n",
    "    a) clean text\n",
    "    b) split sections by headers\n",
    "    \n",
    "4. save csv file\n",
    "    a) PK: NOTEVENTS.ROW_ID\n",
    "    b) TEXT: string(doubled-list)\n",
    "'''\n",
    "def get_discharge_summary(df_notevents):\n",
    "\n",
    "    cond1 = (df_notevents.CATEGORY == 'Discharge summary')\n",
    "    cond2 = (df_notevents.DESCRIPTION == 'Report')\n",
    "\n",
    "    df_discharge_smmary = df_notevents[cond1&cond2]\n",
    "    df_discharge_smmary = df_discharge_smmary[['ROW_ID', 'TEXT']]\n",
    "    \n",
    "    # eliminate a short-note (subject_id=30561, hadm_id=178941)\n",
    "    df_discharge_smmary = df_discharge_smmary[df_discharge_smmary.TEXT.apply(lambda x: len(x) > 100)]\n",
    "\n",
    "    return df_discharge_smmary\n",
    "\n",
    "\n",
    "def pattern_repl(matchobj):\n",
    "    # Return a replacement string to be used for match object\n",
    "    return ' '.rjust(len(matchobj.group(0)))  \n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Replace [**Patterns**] with spaces.\n",
    "    text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', pattern_repl, text)\n",
    "    \n",
    "    # 2. Replace `_` with spaces.\n",
    "    new_text = re.sub(r'_', ' ', text)\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "\n",
    "def split_section(text):\n",
    "    headers, sections = [], []\n",
    "#     pattern = \"^([A-z0-9 ]+)(:)|Discharge Date:|Sex:|JOB#:|Unit No:|FOLLOW-UP PLANS:\"\n",
    "    except_pattern = \"(?!(Sig:)|(disp:))\"\n",
    "    include_keywords = \"(Discharge Date:)|(Sex:)|(JOB#:)|(Unit No:)|(FOLLOW-UP PLANS:)\"\n",
    "    pattern = \"^\" + except_pattern + \"([A-z0-9 ]+)(:)|\" + include_keywords\n",
    "    SEPERATORS = re.compile(pattern, re.I | re.M)\n",
    "    start = 0\n",
    "    \n",
    "    for matcher in SEPERATORS.finditer(text):\n",
    "        # cut off by the position of later SEPERATOR\n",
    "        end = matcher.start()\n",
    "        if end != start: # except for first line\n",
    "            section = text[start:end]\n",
    "            if ':' not in section: #\n",
    "                pass\n",
    "            else:\n",
    "                section = section[len(header):].strip() # except for header in section\n",
    "                sections.append(section)\n",
    "        start = end\n",
    "        end = matcher.end()\n",
    "        \n",
    "        # collect each title in the beginning of section\n",
    "        header = text[start:end].lower()\n",
    "        headers.append(header)\n",
    "        \n",
    "    # add last section\n",
    "    section = text[start:]\n",
    "    section = section[len(header):].strip()\n",
    "    sections.append(section)\n",
    "    \n",
    "    return headers, sections\n",
    "\n",
    "\n",
    "def clean_header(header):\n",
    "    # delete : (colon)\n",
    "    header = re.sub(r',', '', header)\n",
    "    new_header = re.sub(r':', '', header)\n",
    "    new_header = new_header.strip()\n",
    "    return new_header\n",
    "\n",
    "\n",
    "def clean_section(section):\n",
    "    # Replace multiple spaces with a space.\n",
    "    new_section = ' '.join(section.split())\n",
    "    return new_section\n",
    "\n",
    "\n",
    "def preprocess_discharge_summary(text):\n",
    "    text = clean_text(text)\n",
    "    headers, sections = split_section(text)\n",
    "    \n",
    "    new_headers, new_sections = [], []\n",
    "    for idx in range(len(headers)):\n",
    "        h = clean_header(headers[idx])\n",
    "        s = clean_section(sections[idx])\n",
    "        new_headers.append(h)\n",
    "        new_sections.append(s)\n",
    "    return [new_headers, new_sections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load NOTEEVENTS successfully!\n",
      "Get discharge summary successfully!\n",
      "Preprocess notes successfully!\n",
      "save successfully!\n"
     ]
    }
   ],
   "source": [
    "LOAD_FILE_PATH = 'mimic_table/NOTEEVENTS.csv'\n",
    "SAVE_FILE_PATH = os.path.join(ROOT_PATH, 'NOTEEVENTS_SECTIONS.csv')\n",
    "\n",
    "\n",
    "def main_notes():\n",
    "    \n",
    "    data = load_noteevents(file_path=LOAD_FILE_PATH)\n",
    "    print('Load NOTEEVENTS successfully!')\n",
    "    \n",
    "    data = get_discharge_summary(data)\n",
    "    print('Get discharge summary successfully!')\n",
    "    \n",
    "    notes = data.TEXT.apply(lambda x: json.dumps(preprocess_discharge_summary(x)))\n",
    "    print('Preprocess notes successfully!')\n",
    "    \n",
    "    new_data = pd.concat([data.ROW_ID, notes], axis=1)\n",
    "    save_csv_file(csv_data=new_data, file_path=SAVE_FILE_PATH)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main_notes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. extract xx_sections\n",
    "\n",
    "**input:**\n",
    "- result/NOTEEVENTS_SECTIONS.csv  \n",
    "\n",
    "**output:**\n",
    "- result/xx_sections.csv\n",
    "    - (1) px = procedures\n",
    "    - (2) dx = diagnosis\n",
    "    - (3) rx = prescriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PX_HEADERS = ['major surgical or invasive procedure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sections --> PX_Sections\n",
    "\n",
    "1. load NOTEEVENTS_SECTIONS.csv\n",
    "\n",
    "2. extract px_section\n",
    "    a) find px_section by finding PX_HEADERS\n",
    "        - for now, just support the following\n",
    "        'major surgical or invasive procedure' (38374)\n",
    "        \n",
    "3. preprocess px_section\n",
    "    a) drop NA in header part\n",
    "    b) drop NA in section part\n",
    "    c) drop 'None'/'none'\n",
    "    \n",
    "4. save csv file\n",
    "    a) PK: NOTEVENTS.ROW_ID\n",
    "    b) TEXT\n",
    "'''\n",
    "\n",
    "\n",
    "# for now, not support multiple tgt_headers\n",
    "def extract_px_section(text: str, tgt_headers=PX_HEADERS) -> List[List[str]]:\n",
    "    \n",
    "    text = json.loads(text) # change string format to dict\n",
    "    headers, sections = text[0], text[1]\n",
    "    \n",
    "    px_pos = []\n",
    "    px_headers, px_sections = [], []\n",
    "    \n",
    "    for idx in range(len(headers)):\n",
    "        h = headers[idx]\n",
    "        if h in tgt_headers:\n",
    "            pos = headers.index(h)\n",
    "            px_pos.append(pos)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    if len(px_pos) > 0:\n",
    "        for pos in sorted(px_pos):\n",
    "            px_headers.append(headers[pos])\n",
    "            px_sections.append(sections[pos])\n",
    "\n",
    "    return [px_headers, px_sections]\n",
    "\n",
    "\n",
    "# def extract_prx_section(text):\n",
    "#     prx_section = []\n",
    "#     text = json.loads(text) # change string format to dict\n",
    "#     headers, sections = text[0], text[1]\n",
    "    \n",
    "#     query = 'major surgical or invasive procedure'\n",
    "#     try:\n",
    "#         pos = headers.index(query)\n",
    "#     except:\n",
    "#         pos = \"\"\n",
    "        \n",
    "#     if pos:\n",
    "#         prx_section = sections[pos]\n",
    "            \n",
    "#     return prx_section\n",
    "\n",
    "\n",
    "def preprocess_px_sections(px_sections: pd.Series) -> pd.Series:\n",
    "    \n",
    "    # drop NA\n",
    "    px_sections = px_sections[~px_sections.apply(lambda x: len(x[0]) == 0)]\n",
    "    \n",
    "    # drop NA\n",
    "    px_sections = px_sections[~px_sections.apply(lambda x: len(''.join(x[1])) == 0)]\n",
    "    \n",
    "    # drop NA string form like 'None', 'none'\n",
    "    string_NA = [['None'], ['none'], ['NA']]\n",
    "    px_sections = px_sections[~px_sections.apply(lambda x: x[1] in string_NA)]\n",
    "    \n",
    "    return px_sections\n",
    "        \n",
    "    \n",
    "# def eda_px_section(notes: pd.Series, option='cnt') -> int:\n",
    "#     notes = notes.apply(lambda x: extract_px_section(text=x))\n",
    "#     if option == 'cnt':\n",
    "#         output = notes.apply(lambda x: len(x) > 0).sum()\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load NOTEEVENTS_SECTIONS successfully! 55176\n",
      "Extract px sections succesfully! 55176\n",
      "Preprocess px sections successfully! 27989\n",
      "save successfully!\n"
     ]
    }
   ],
   "source": [
    "LOAD_FILE_PATH = os.path.join(ROOT_PATH, 'NOTEEVENTS_SECTIONS.csv')\n",
    "SAVE_FILE_PATH = os.path.join(ROOT_PATH, 'px_sections.csv')\n",
    "\n",
    "\n",
    "def main_px():\n",
    "    \n",
    "    data = load_csv_file(file_path=LOAD_FILE_PATH)\n",
    "    print('Load NOTEEVENTS_SECTIONS successfully!', len(data))\n",
    "    \n",
    "    notes = data['TEXT'].apply(lambda x: extract_px_section(x))\n",
    "    print('Extract px sections succesfully!', len(notes))\n",
    "    \n",
    "    notes = preprocess_px_sections(notes)\n",
    "    print('Preprocess px sections successfully!', len(notes))\n",
    "    \n",
    "    new_data   = pd.concat([data.ROW_ID, notes], axis=1)\n",
    "    save_csv_file(csv_data=new_data, file_path=SAVE_FILE_PATH)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main_px()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "DX_HEADERS = [\n",
    "    'discharge diagnosis',  # 41335 \n",
    "    'discharge diagnoses', # 7615\n",
    "    'primary diagnosis', # 2796\n",
    "    'primary diagnoses',\n",
    "    'primary dx',\n",
    "    'primary', # 4869\n",
    "    'secondary diagnosis', # 1800\n",
    "    'secondary diagnoses',\n",
    "    'secondary', # 4402\n",
    "    'secondary dx',\n",
    "    'secondary dianogsis',\n",
    "    'final diagnosis',  # 1691\n",
    "    'other diagnosis'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sections --> DX_Sections\n",
    "\n",
    "1. load NOTEEVENTS_SECTIONS.csv\n",
    "\n",
    "2. extract dx_section\n",
    "    a) find dx_section by finding DX_HEADERS\n",
    "    b) (for now) must satisfy that last dx_section be right before 'discharge conditon'\n",
    "        \n",
    "3. preprocess dx_section\n",
    "    a) drop NA in header part\n",
    "    b) drop NA in section part\n",
    "    c) drop 'None'/'none'\n",
    "    \n",
    "4. save csv file\n",
    "    a) PK: NOTEVENTS.ROW_ID\n",
    "    b) TEXT\n",
    "'''\n",
    "    \n",
    "\n",
    "def extract_dx_section(text: str, tgt_headers=DX_HEADERS) -> List[List[str]]:\n",
    "    \n",
    "    text = json.loads(text) # change string format to dict\n",
    "    headers, sections = text[0], text[1]\n",
    "    \n",
    "    dx_pos = []\n",
    "    dx_headers, dx_sections = [], []\n",
    "    \n",
    "    pos_next = -999\n",
    "    h_next = ['discharge condition']\n",
    "    \n",
    "    for idx in range(len(headers)):\n",
    "        h = headers[idx]\n",
    "        if h in tgt_headers:\n",
    "            pos = headers.index(h)\n",
    "            dx_pos.append(pos) \n",
    "#             dx_headers.append(headers[pos])\n",
    "#             dx_sections.append(sections[pos])\n",
    "        if h in h_next:\n",
    "            pos_next = headers.index(h_next[0])\n",
    "    \n",
    "    if len(dx_pos) > 0:\n",
    "        for pos in sorted(dx_pos):\n",
    "            dx_headers.append(headers[pos])\n",
    "            dx_sections.append(sections[pos])\n",
    "            \n",
    "        if pos_next == (max(dx_pos)+1):\n",
    "            pass\n",
    "        else: \n",
    "            dx_headers = []\n",
    "            dx_sections = []\n",
    "    \n",
    "    return [dx_headers, dx_sections]\n",
    "\n",
    "\n",
    "def preprocess_dx_sections(dx_sections: pd.Series) -> pd.Series:\n",
    "    \n",
    "    # drop NA in header part\n",
    "    dx_sections = dx_sections[~dx_sections.apply(lambda x: len(x[0]) == 0)]\n",
    "    \n",
    "    # drop NA in section part\n",
    "    dx_sections = dx_sections[~dx_sections.apply(lambda x: len(''.join(x[1])) == 0)]\n",
    "    \n",
    "    # drop NA string form like 'None', 'none'\n",
    "    string_NA = [['None'], ['none'], ['NA']]\n",
    "    dx_sections = dx_sections[~dx_sections.apply(lambda x: x[1] in string_NA)]\n",
    "    \n",
    "    return dx_sections\n",
    "\n",
    "\n",
    "# def eda_dx_section(notes: pd.Series, option='cnt') -> int:\n",
    "#     notes = notes.apply(lambda x: extract_dx_section(text=x))\n",
    "#     if option == 'cnt':\n",
    "#         output = notes.apply(lambda x: len(x[0]) > 0).sum()\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load NOTEEVENTS_SECTIONS successfully! 55176\n",
      "Extract dx sections succesfully! 55176\n",
      "Preprocess dx sections successfully! 36015\n",
      "save successfully!\n"
     ]
    }
   ],
   "source": [
    "LOAD_FILE_PATH = os.path.join(ROOT_PATH, 'NOTEEVENTS_SECTIONS.csv')\n",
    "SAVE_FILE_PATH = os.path.join(ROOT_PATH, 'dx_sections.csv')\n",
    "\n",
    "\n",
    "def main_dx():\n",
    "    \n",
    "    data = load_csv_file(file_path=LOAD_FILE_PATH)\n",
    "    print('Load NOTEEVENTS_SECTIONS successfully!', len(data))\n",
    "    \n",
    "    notes = data.TEXT.apply(lambda x: extract_dx_section(x))\n",
    "    print('Extract dx sections succesfully!', len(notes))\n",
    "    \n",
    "    notes = preprocess_dx_sections(notes)\n",
    "    print('Preprocess dx sections successfully!', len(notes))\n",
    "    \n",
    "    new_data   = pd.concat([data.ROW_ID, notes], axis=1)\n",
    "    save_csv_file(csv_data=new_data, file_path=SAVE_FILE_PATH)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main_dx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. rx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "RX_HEADERS = ['discharge medications']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sections --> RX_Sections\n",
    "\n",
    "1. load NOTEEVENTS_SECTIONS.csv\n",
    "\n",
    "2. extract rx_section\n",
    "        \n",
    "3. preprocess rx_section\n",
    "    \n",
    "4. save csv file\n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "# def extract_rx_section(text: str, tgt_headers=RX_HEADERS) -> List[List[str]]:\n",
    "    \n",
    "#     text = json.loads(text) # change string format to dict\n",
    "#     headers, sections = text[0], text[1]\n",
    "    \n",
    "#     rx_headers, rx_sections = [], []\n",
    "    \n",
    "    \n",
    "#     h_next = ['discharge condition']\n",
    "#     pos_next = - 99999\n",
    "    \n",
    "#     for idx in range(len(headers)):\n",
    "#         h = headers[idx]\n",
    "#         if h in tgt_headers:\n",
    "#             pos = headers.index(h)\n",
    "#             dx_pos.append(pos) \n",
    "#             dx_headers.append(headers[pos])\n",
    "#             dx_sections.append(sections[pos])\n",
    "#         if h in h_next:\n",
    "#             pos_next = headers.index(h_next[0])\n",
    "    \n",
    "#     if len(dx_pos) > 0:\n",
    "#         if pos_next == max(dx_pos)+1 :\n",
    "#             pass\n",
    "#         else: \n",
    "#             dx_headers = []\n",
    "#             dx_sections = []\n",
    "    \n",
    "#     return [dx_headers, dx_sections]\n",
    "\n",
    "    \n",
    "def extract_rx_section(text:str) -> List[List[str]]:\n",
    "    \n",
    "    text = json.loads(text) # change string format to dict\n",
    "    headers, sections = text[0], text[1]\n",
    "    \n",
    "    rx_headers, rx_sections = [], []\n",
    "    \n",
    "    pos1, pos2, pos3, pos4 = -999, -999, -999, -999\n",
    "    \n",
    "    h1 = 'discharge medications'\n",
    "    h2 = 'discharge disposition'\n",
    "    h3 = 'discharge diagnosis'\n",
    "    h4 = 'discharge condition'\n",
    "    \n",
    "    if h1 in headers:\n",
    "        pos1 = headers.index(h1)\n",
    "    if h2 in headers:\n",
    "        pos2 = headers.index(h2)\n",
    "    if h3 in headers:\n",
    "        pos3 = headers.index(h3)\n",
    "    if h4 in headers:\n",
    "        pos4 = headers.index(h4)\n",
    "\n",
    "    if pos1 + pos2 + pos3 + pos4 > 0: # have all together\n",
    "        if pos1 < pos2 < pos3 < pos4: # well organized\n",
    "            rx_headers = headers[pos1:pos2]\n",
    "            rx_sections = sections[pos1:pos2]\n",
    "#             rx_section = ' '.join(sections[pos1:pos2])\n",
    "            \n",
    "    return [rx_headers, rx_sections]\n",
    "\n",
    "\n",
    "def preprocess_rx_sections(rx_sections: pd.Series) -> pd.Series:\n",
    "    \n",
    "    # drop NA in header part\n",
    "    rx_sections = rx_sections[~rx_sections.apply(lambda x: len(x[0]) == 0)]\n",
    "    \n",
    "    # drop NA in section part\n",
    "    rx_sections = rx_sections[~rx_sections.apply(lambda x: len(''.join(x[1])) == 0)]\n",
    "    \n",
    "    # drop NA string form like 'None', 'none'\n",
    "    string_NA = [['None'], ['none'], ['NA']]\n",
    "    rx_sections = rx_sections[~rx_sections.apply(lambda x: x[1] in string_NA)]\n",
    "    \n",
    "    return rx_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load NOTEEVENTS_SECTIONS successfully! 55176\n",
      "Extract rx sections succesfully! 55176\n",
      "Preprocess rx sections successfully! 35149\n",
      "save successfully!\n"
     ]
    }
   ],
   "source": [
    "LOAD_FILE_PATH = os.path.join(ROOT_PATH, 'NOTEEVENTS_SECTIONS.csv')\n",
    "SAVE_FILE_PATH = os.path.join(ROOT_PATH, 'rx_sections.csv')\n",
    "\n",
    "\n",
    "def main_rx():\n",
    "    \n",
    "    data = load_csv_file(file_path=LOAD_FILE_PATH)\n",
    "    print('Load NOTEEVENTS_SECTIONS successfully!', len(data))\n",
    "    \n",
    "    notes = data['TEXT'].apply(lambda x: extract_rx_section(x))\n",
    "    print('Extract rx sections succesfully!', len(notes))\n",
    "    \n",
    "    notes = preprocess_rx_sections(notes)\n",
    "    print('Preprocess rx sections successfully!', len(notes))\n",
    "    \n",
    "    new_data   = pd.concat([data.ROW_ID, notes], axis=1)\n",
    "    save_csv_file(csv_data=new_data, file_path=SAVE_FILE_PATH)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main_rx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tokenize by scispacy and recover hadm_ids\n",
    "\n",
    "**input:**\n",
    "- result/xx_sections.csv\n",
    "\n",
    "**output:**\n",
    "- result/xx_sections.txt, result/xx_hadm_ids.txt\n",
    "    - (1) px = procedures\n",
    "    - (2) dx = diagnosis\n",
    "    - (3) rx = prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XX_SECTIONS = ['px', 'dx', 'rx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import spacy, scispacy\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "\n",
    "def save_txt_file(txt_file, save_file_path):\n",
    "    with open(save_file_path, \"w\") as file:\n",
    "        for txt in txt_file:\n",
    "            file.write(txt)\n",
    "            file.write('\\n')\n",
    "            file.write('\\n')\n",
    "    return print('save successfully!')\n",
    "\n",
    "\n",
    "def preprocess_scispacy(nlp, section):\n",
    "    tokenized_section = ' '.join([token.text for token in nlp(section)])\n",
    "    return tokenized_section\n",
    "\n",
    "\n",
    "# def recover_hadm_ids_from_noteevents(row_ids):\n",
    "    \n",
    "#     # load NOTEEVENTS.csv\n",
    "#     noteevents = load_noteevents(file_path=NOTE_PATH)\n",
    "#     noteevents = noteevents[['ROW_ID', 'HADM_ID']]\n",
    "    \n",
    "#     # convert ROW_ID to HADM_ID\n",
    "#     hadm_ids = noteevents[noteevents['ROW_ID'].isin(target_df['ROW_ID'])].HADM_ID\n",
    "#     hadm_ids = hadm_ids.astype(int).astype(str)\n",
    "#     return hadm_ids\n",
    "\n",
    "\n",
    "def convert_as_textual_data_form(data: str, xx_type: str):\n",
    "    \n",
    "    textual_data = ''\n",
    "    \n",
    "    # convert string to doubled-list\n",
    "    text = ast.literal_eval(data)\n",
    "    \n",
    "    headers, sections = text[0], text[1]\n",
    "    n_headers = len(headers)\n",
    "    \n",
    "    if xx_type == 'px':\n",
    "        # n_headers = 1\n",
    "        textual_data = sections[0]\n",
    "        \n",
    "    elif xx_type == 'dx':\n",
    "        textual_data = ' '.join(sections) \n",
    "    \n",
    "    else: # rx\n",
    "        textual_data += (sections[0] + ' ')\n",
    "        for idx in range(1, n_headers):\n",
    "            textual_data += (headers[idx] + ' ')\n",
    "            textual_data += (sections[idx] + ' ')\n",
    "    return textual_data\n",
    "\n",
    "\n",
    "def recover_hadm_ids(data_row_id: pd.Series, save_file_path: str):\n",
    "    \n",
    "     # load NOTEEVENTS.csv\n",
    "    noteevents = load_noteevents(file_path=NOTE_PATH)\n",
    "    noteevents = noteevents[['ROW_ID', 'HADM_ID']]\n",
    "    \n",
    "    # convert ROW_ID to HADM_ID\n",
    "    hadm_ids = noteevents[noteevents['ROW_ID'].isin(data_row_id)].HADM_ID\n",
    "    hadm_ids = hadm_ids.astype(int).astype(str)\n",
    "    \n",
    "    save_txt_file(txt_file=hadm_ids, save_file_path=save_file_path)\n",
    "    print('save hadm_id.txt successfully!!', len(hadm_ids))\n",
    "\n",
    "\n",
    "def tokenize_scispacy_sections(data_text: pd.Series, save_file_path: str):\n",
    "\n",
    "    # tokenized by scispacy\n",
    "    nlp = spacy.load(\"en_core_sci_sm\")\n",
    "    \n",
    "    data_text_tokenized = data_text.copy()\n",
    "#     data.TEXT = data_t.apply(lambda x: preprocess_scispacy(nlp, x))\n",
    "    for i in trange(len(data_text)):\n",
    "        data_text_tokenized.iloc[i] = preprocess_scispacy(nlp, data_text.iloc[i])\n",
    "    \n",
    "    # save section corpus\n",
    "    save_txt_file(txt_file=data_text_tokenized, save_file_path=save_file_path)\n",
    "    print('save section.txt successfully!!', len(data_text_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seongsu/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save successfully!\n",
      "save hadm_id.txt successfully!! 27989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27989/27989 [04:11<00:00, 111.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save successfully!\n",
      "save section.txt successfully!! 27989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NOTE_PATH = os.path.join('mimic_table','NOTEEVENTS.csv')\n",
    "LOAD_FILE_PATH = os.path.join(ROOT_PATH, 'px_sections.csv')\n",
    "ADM_SAVE_FILE_PATH = os.path.join(ROOT_PATH, 'px_hadm_ids.txt')\n",
    "SEC_SAVE_FILE_PATH = os.path.join(ROOT_PATH, 'px_sections.txt')\n",
    "\n",
    "\n",
    "def preprocess_px_sections_csv_files(load_file_path: str):\n",
    "    \n",
    "    data = load_csv_file(file_path=load_file_path)  # load data\n",
    "    data = data[data['TEXT'].notna()]  # drop na\n",
    "    data_text = data['TEXT'].apply(lambda x: convert_as_textual_data_form(data=x, xx_type='px'))  # convert\n",
    "    \n",
    "    data_row_id = data['ROW_ID']\n",
    "    \n",
    "    return data_row_id, data_text\n",
    "\n",
    "\n",
    "def main_px_2():\n",
    "    data_r, data_t = preprocess_px_sections_csv_files(load_file_path=LOAD_FILE_PATH)\n",
    "    recover_hadm_ids(data_row_id=data_r, save_file_path=ADM_SAVE_FILE_PATH)\n",
    "    tokenize_scispacy_sections(data_text=data_t, save_file_path=SEC_SAVE_FILE_PATH)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main_px_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seongsu/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save successfully!\n",
      "save hadm_id.txt successfully!! 36015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36015/36015 [06:05<00:00, 98.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save successfully!\n",
      "save section.txt successfully!! 36015\n"
     ]
    }
   ],
   "source": [
    "NOTE_PATH = os.path.join('mimic_table','NOTEEVENTS.csv')\n",
    "LOAD_FILE_PATH = os.path.join(ROOT_PATH, 'dx_sections.csv')\n",
    "ADM_SAVE_FILE_PATH = os.path.join(ROOT_PATH, 'dx_hadm_ids.txt')\n",
    "SEC_SAVE_FILE_PATH = os.path.join(ROOT_PATH, 'dx_sections.txt')\n",
    "\n",
    "\n",
    "def preprocess_dx_sections_csv_files(load_file_path):\n",
    "    \n",
    "    data = load_csv_file(file_path=load_file_path)  # load data\n",
    "    data = data[data['TEXT'].notna()]  # drop na\n",
    "    data_text = data['TEXT'].apply(lambda x: convert_as_textual_data_form(data=x, xx_type='dx'))  # convert\n",
    "\n",
    "    data_row_id = data['ROW_ID']\n",
    "    \n",
    "    return data_row_id, data_text\n",
    "\n",
    "    \n",
    "def main_dx_2():\n",
    "    data_r, data_t = preprocess_dx_sections_csv_files(load_file_path=LOAD_FILE_PATH)\n",
    "    recover_hadm_ids(data_row_id=data_r, save_file_path=ADM_SAVE_FILE_PATH)\n",
    "    tokenize_scispacy_sections(data_text=data_t, save_file_path=SEC_SAVE_FILE_PATH)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main_dx_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. rx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seongsu/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save successfully!\n",
      "save hadm_id.txt successfully!! 32218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32218/32218 [24:45<00:00, 21.68it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save successfully!\n",
      "save section.txt successfully!! 32218\n"
     ]
    }
   ],
   "source": [
    "NOTE_PATH = os.path.join('mimic_table','NOTEEVENTS.csv')\n",
    "LOAD_FILE_PATH = os.path.join(ROOT_PATH, 'rx_sections.csv')\n",
    "ADM_SAVE_FILE_PATH = os.path.join(ROOT_PATH, 'rx_hadm_ids.txt')\n",
    "SEC_SAVE_FILE_PATH = os.path.join(ROOT_PATH, 'rx_sections.txt')\n",
    "\n",
    "\n",
    "def preprocess_rx_sections_csv_files(load_file_path):\n",
    "    \n",
    "    data = load_csv_file(file_path=load_file_path)  # load data\n",
    "    data = data[data['TEXT'].notna()]  # drop na\n",
    "    data_text = data['TEXT'].apply(lambda x: convert_as_textual_data_form(data=x, xx_type='rx'))  # convert\n",
    "    \n",
    "    # length check\n",
    "    data = data[data_text.apply(lambda x: len(x) > 200)]\n",
    "    \n",
    "    data_row_id = data['ROW_ID']\n",
    "    data_text = data['TEXT']\n",
    "    \n",
    "    return data_row_id, data_text\n",
    "\n",
    "    \n",
    "def main_rx_2():\n",
    "    data_r, data_t = preprocess_rx_sections_csv_files(load_file_path=LOAD_FILE_PATH)\n",
    "    recover_hadm_ids(data_row_id=data_r, save_file_path=ADM_SAVE_FILE_PATH)\n",
    "    tokenize_scispacy_sections(data_text=data_t, save_file_path=SEC_SAVE_FILE_PATH)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main_rx_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def main():\n",
    "#     data       = load_csv_file(file_path=LOAD_FILE_PATH)\n",
    "#     # data = data.iloc[:10]\n",
    "    \n",
    "#     # not na \n",
    "#     data = data[data.TEXT.notna()]\n",
    "\n",
    "#     # length > 200\n",
    "#     data = data[data.TEXT.apply(lambda x: len(x) > 200)]\n",
    "\n",
    "#     # delete \"\"\n",
    "#     data1 = data.copy()\n",
    "#     data1.TEXT = data.TEXT.apply(lambda x: x[1:-1])\n",
    "\n",
    "#     # preprocessed by scispacy\n",
    "#     nlp = spacy.load(\"en_core_sci_sm\")\n",
    "#     data.TEXT = data1.TEXT.apply(lambda x: preprocess_scispacy(nlp, x))\n",
    "#     del data1\n",
    "#     print('preprocess successfully!')\n",
    "\n",
    "#     # recover and extract full info of data(subject_id, hamd_id)\n",
    "#     noteevents = load_csv_file(file_path=NOTE_PAHT)\n",
    "#     noteevents = noteevents[['ROW_ID', 'SUBJECT_ID', 'HADM_ID']]\n",
    "\n",
    "#     # data=p / noteevents\n",
    "#     data1 = noteevents[noteevents.ROW_ID.isin(data.ROW_ID)]\n",
    "    \n",
    "#     # save txt file\n",
    "#     print('data len: {}', len(data))\n",
    "#     save_txt_file(txt_file=data.TEXT, file_path=SEC_SAVE_FILE_PATH)\n",
    "#     hadm_id = data1.HADM_ID.astype(int).astype(str)\n",
    "#     print('data1 len: {}', len(hadm_id))\n",
    "#     save_txt_file(txt_file=hadm_id, file_path=ADM_SAVE_FILE_PATH)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
