{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "triple2subgraph.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME6dlj5hrbVR"
      },
      "source": [
        "**0. Code for Colab Debugging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3dNWgM2EgvW",
        "outputId": "0a8e0fc8-97c0-48f0-eeca-7115897bad8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My Drive/lxmert/data/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/lxmert/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCzMtobduJtf"
      },
      "source": [
        "**1-1. Run Depth First Search on KG (Node only)**. Root is an admission node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6P8d-FytHkS",
        "outputId": "535c80ac-9331-4297-9e8c-49ce8dbb5bf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "NUM_SPECIAL_TOKENS = 2\n",
        "\n",
        "def get_childs(subgraph, depth, heads):\n",
        "    temp_seq = list()\n",
        "    for head in heads:\n",
        "        temp_seq += subgraph[depth][head]\n",
        "    return temp_seq\n",
        "\n",
        "triples = [x.split() for x in open('train2id.txt').read().splitlines()[1:]]\n",
        "node2edge = {(h,t):r for h,t,r in triples}\n",
        "nodes = {' '.join(x.split()[:-1]):x.split()[-1] for x in open('entity2id.txt').read().splitlines()[1:]}\n",
        "literals = {k:int(v)+NUM_SPECIAL_TOKENS for (k,v) in list(nodes.items()) if '^^' in k}\n",
        "edges = {x.split()[0]:x.split()[1] for x in open('relation2id.txt').read().splitlines()[1:]}\n",
        "\n",
        "# Extract Admission Nodes & Literals\n",
        "adm_node = list()\n",
        "for node in list(nodes.items()):\n",
        "    if 'hadm' in node[0]:\n",
        "        adm_node.append(node[1])\n",
        "        \n",
        "# Initialize subgraph\n",
        "subgraph_norel = [{node:list() for node in adm_node}]\n",
        "\n",
        "#subgraph_rel = dict(adm_node)\n",
        "#node_dict = list(subgraph_norel.keys())\n",
        "\n",
        "# Depth First Search\n",
        "print('start preprocessing')\n",
        "level = 0\n",
        "while len(triples)>0:\n",
        "    queue = list()\n",
        "    print('level:{}'.format(level))\n",
        "    for triple in tqdm(triples):\n",
        "        if triple[0] in subgraph_norel[level]:\n",
        "            subgraph_norel[level][triple[0]].append(triple[1])\n",
        "            flag = False\n",
        "        else:\n",
        "            flag = True\n",
        "        if flag:\n",
        "            queue.append(triple)\n",
        "    print('{}/{}'.format(len(queue),len(triples)))\n",
        "    new_head = list()\n",
        "    for heads in list(subgraph_norel[level].values()):\n",
        "        new_head+=heads\n",
        "    subgraph_norel.append({k:list() for k in new_head})\n",
        "    triples = queue\n",
        "    level += 1\n",
        "\n",
        "# Build subgraph\n",
        "subgraphs = dict()\n",
        "max_len = 900\n",
        "cnt = 0\n",
        "for head in tqdm(list(subgraph_norel[0].keys())):\n",
        "    depth=0\n",
        "    seq = [head]\n",
        "    heads = [head]\n",
        "    while depth<level:\n",
        "        heads = get_childs(subgraph_norel,depth,heads)\n",
        "        seq += heads\n",
        "        depth+=1\n",
        "    if len(seq)<=max_len:\n",
        "        subgraphs[head]=[int(x)+NUM_SPECIAL_TOKENS for x in seq]+[0]*(max_len-len(seq))\n",
        "\n",
        "# Align subgraph and note\n",
        "aid = [nodes['</hadm_id/{}>'.format(x)] for x in open('p_hadm_ids.txt').read().splitlines() if (len(x)>0) and ('</hadm_id/{}>'.format(x) in nodes)]\n",
        "note = [x for x in open('p_sections.txt').read().splitlines() if (len(x)>0)]\n",
        "note_aid_pair = [(x,y) for (x,y) in zip(aid,note) if x in subgraphs]\n",
        "print('{}/{}'.format(len(aid),len(adm_node)))\n",
        "print(len(note))\n",
        "print(len(note_aid_pair))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 117834/18755503 [00:00<00:15, 1178327.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "start preprocessing\n",
            "level:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18755503/18755503 [00:13<00:00, 1434326.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15929096/18755503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 55333/15929096 [00:00<00:28, 553326.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "level:1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15929096/15929096 [00:29<00:00, 544815.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0/15929096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31969/31969 [00:08<00:00, 3647.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32202/31969\n",
            "32215\n",
            "27075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7awNjSEwl9E"
      },
      "source": [
        "**1-2. Run Depth First Search on KG (Node & Relation)**. Root is an admission node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qME9ZZ1UwuSv"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "NUM_SPECIAL_TOKENS = 2\n",
        "\n",
        "def get_childs_withrel(subgraph, depth, heads,node2edge):\n",
        "    temp_seq = list()\n",
        "    temp_heads = list()\n",
        "    for head in heads:\n",
        "        node_set = [(head,tail) for tail in subgraph[depth][head]]\n",
        "        for node_pair in node_set:\n",
        "            temp_seq += ['r'+node2edge[node_pair],node_pair[1]]\n",
        "        temp_heads += subgraph[depth][head]\n",
        "    return temp_seq, temp_heads\n",
        "\n",
        "triples = [x.split() for x in open('train2id.txt').read().splitlines()[1:]]\n",
        "node2edge = {(h,t):r for h,t,r in triples}\n",
        "nodes = {' '.join(x.split()[:-1]):x.split()[-1] for x in open('entity2id.txt').read().splitlines()[1:]}\n",
        "literals = {k:int(v)+NUM_SPECIAL_TOKENS for (k,v) in list(nodes.items()) if '^^' in node[0]}\n",
        "edges = {x.split()[0]:x.split()[1] for x in open('relation2id.txt').read().splitlines()[1:]}\n",
        "\n",
        "# Extract Admission Nodes & Literals\n",
        "adm_node = list()\n",
        "for node in list(nodes.items()):\n",
        "    if 'hadm' in node[0]:\n",
        "        adm_node.append(node[1])   \n",
        "        \n",
        "# Initialize subgraph\n",
        "subgraph_norel = [{node:list() for node in adm_node}]\n",
        "\n",
        "#subgraph_rel = dict(adm_node)\n",
        "#node_dict = list(subgraph_norel.keys())\n",
        "\n",
        "# Depth First Search\n",
        "print('start preprocessing')\n",
        "level = 0\n",
        "while len(triples)>0:\n",
        "    queue = list()\n",
        "    print('level:{}'.format(level))\n",
        "    for triple in tqdm(triples):\n",
        "        if triple[0] in subgraph_norel[level]:\n",
        "            subgraph_norel[level][triple[0]].append(triple[1])\n",
        "            flag = False\n",
        "        else:\n",
        "            flag = True\n",
        "        if flag:\n",
        "            queue.append(triple)\n",
        "    print('{}/{}'.format(len(queue),len(triples)))\n",
        "    new_head = list()\n",
        "    for heads in list(subgraph_norel[level].values()):\n",
        "        new_head+=heads\n",
        "    subgraph_norel.append({k:list() for k in new_head})\n",
        "    triples = queue\n",
        "    level += 1\n",
        "\n",
        "# Build subgraph\n",
        "subgraphs = list()\n",
        "max_len = 0\n",
        "for head in tqdm(list(subgraph_norel[0].keys())):\n",
        "    depth=0\n",
        "    seq = [head]\n",
        "    heads = [head]\n",
        "    while depth<level:\n",
        "        seqs, heads = get_childs_withrel(subgraph_norel,depth,heads,node2edge)\n",
        "        seq += seqs\n",
        "        depth+=1\n",
        "    subgraphs.append(list(map(lambda x: int(x)+NUM_SPECIAL_TOKENS if 'r' not in x else -(int(x.split('r')[-1])+1),seq)))\n",
        "    if len(seq)>max_len:\n",
        "        max_len = len(seq)\n",
        "\n",
        "\n",
        "# Align subgraph and note\n",
        "aid = [nodes['</hadm_id/{}>'.format(x)] for x in open('p_hadm_ids.txt').read().splitlines() if (len(x)>0) and ('</hadm_id/{}>'.format(x) in nodes)]\n",
        "note = [x for x in open('p_sections.txt').read().splitlines() if (len(x)>0)]\n",
        "note_aid_pair = [(x,y) for (x,y) in zip(aid,note) if x in subgraphs]\n",
        "print('{}/{}'.format(len(aid),len(adm_node)))\n",
        "print(len(note))\n",
        "print(len(note_aid_pair))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIoqHq_nueia"
      },
      "source": [
        "**2-1. Return DB for Masked Literal Pred**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8yG81c8tHkW",
        "outputId": "2b36e7fd-a267-4346-d04c-fb5d0fe2b7b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "task = 'masked_literal_prediction'\n",
        "if not os.path.isdir(task):\n",
        "    os.mkdir(task)\n",
        "# Build Input\n",
        "DB = {'train':[],'valid':[],'test':[]}\n",
        "for sample in note_aid_pair:\n",
        "    split = np.random.choice(list(DB.keys()),p=[0.8,0.1,0.1])\n",
        "    if (len(split)>0.1*len(note_aid_pair)) and (split in ['valid', 'test']):\n",
        "        split = 'train'\n",
        "    elif (len(split)>0.8*len(note_aid_pair)) and (split in ['train']):\n",
        "        split = np.random.choice(['valid','test'],p=[0.5,0.5])\n",
        "    DB[split].append(sample)\n",
        "\n",
        "# Re-index literals for labeling\n",
        "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
        "\n",
        "for split in DB:\n",
        "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
        "    if not os.path.isdir(os.path.join(task,split)):\n",
        "        os.mkdir(os.path.join(task,split))\n",
        "    torch.save([note for (head,note) in DB[split]],'{}/note'.format(os.path.join(task,split)))\n",
        "    torch.save({'input':[subgraphs[head] for (head,note) in DB[split]],\n",
        "                'mask':[(~np.isin(np.array(subgraphs[head]),list(literals.keys()))).astype(np.int64).tolist() for (head,note) in DB[split]],\n",
        "                'label':[list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraphs[head])) for (head,note) in DB[split]]},\n",
        "               '{}/kg_norel'.format(os.path.join(task,split)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[train] set size : 21653\n",
            "[valid] set size : 2697\n",
            "[test] set size : 2725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo4WacHCveOi"
      },
      "source": [
        "**2-2. Return DB for Literal Bucket Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUWX_Dxg1jCH"
      },
      "source": [
        "task = 'masked_literal_prediction'\n",
        "if not os.path.isdir(task):\n",
        "    os.mkdir(task)\n",
        "# Build Input\n",
        "DB = {'train':[],'valid':[],'test':[]}\n",
        "for sample in note_aid_pair:\n",
        "    split = np.random.choice(list(DB.keys()),p=[0.8,0.1,0.1])\n",
        "    if (len(split)>0.1*len(note_aid_pair)) and (split in ['valid', 'test']):\n",
        "        split = 'train'\n",
        "    elif (len(split)>0.8*len(note_aid_pair)) and (split in ['train']):\n",
        "        split = np.random.choice(['valid','test'],p=[0.5,0.5])\n",
        "    DB[split].append(sample)\n",
        "\n",
        "# Load Bucket ID\n",
        "literalID2bucketID = torch.load('literalID2bucketID')\n",
        "\n",
        "for split in DB:\n",
        "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
        "    if not os.path.isdir(os.path.join(task,split)):\n",
        "        os.mkdir(os.path.join(task,split))\n",
        "    torch.save([note for (head,note) in DB[split]],'{}/note'.format(split))\n",
        "    torch.save({'input':[subgraphs[head] for (head,note) in DB[split]],\n",
        "                'mask':[(~np.isin(np.array(subgraphs[head]),list(literals.keys()))).astype(np.int64).tolist() for (head,note) in DB[split]],\n",
        "                'label':[list(map(lambda x: literalID2bucketID[x] if x in literalID2bucketID else -100,subgraphs[head])) for (head,note) in DB[split]]},\n",
        "               '{}/{}/kg_norel'.format(task,split))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl5zew0jvW75"
      },
      "source": [
        "**2-3. Return DB for Contrastive Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNTHhbQZ1jaU"
      },
      "source": [
        "def negative_sampling(input, mask):\n",
        "\n",
        "task = 'masked_literal_prediction'\n",
        "if not os.path.isdir(task):\n",
        "    os.mkdir(task)\n",
        "# Build Input\n",
        "DB = {'train':[],'valid':[],'test':[]}\n",
        "for sample in note_aid_pair:\n",
        "    split = np.random.choice(list(DB.keys()),p=[0.8,0.1,0.1])\n",
        "    if (len(split)>0.1*len(note_aid_pair)) and (split in ['valid', 'test']):\n",
        "        split = 'train'\n",
        "    elif (len(split)>0.8*len(note_aid_pair)) and (split in ['train']):\n",
        "        split = np.random.choice(['valid','test'],p=[0.5,0.5])\n",
        "    DB[split].append(sample)\n",
        "\n",
        "# Re-index literals for labeling\n",
        "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
        "\n",
        "for split in DB:\n",
        "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
        "    if not os.path.isdir(os.path.join(task,split)):\n",
        "        os.mkdir(os.path.join(task,split))\n",
        "    torch.save([note for (head,note) in DB[split]],'{}/note'.format(split))\n",
        "    torch.save({'input':[subgraphs[head] for (head,note) in DB[split]],\n",
        "                'mask':[(~np.isin(np.array(subgraphs[head]),list(literals.keys()))).astype(np.int64).tolist() for (head,note) in DB[split]],\n",
        "                'label':[list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraphs[head])) for (head,note) in DB[split]]},\n",
        "               '{}/{}/kg_norel'.format(task,split))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66m8nuQDvqUA"
      },
      "source": [
        "**2-4. Return DB for Literal Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIWjye4ZtHkY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3z4_vqY2rzc"
      },
      "source": [
        "**Supp 1. Save DB in torch.tensor format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwxPUhGr2ruw"
      },
      "source": [
        "# Only for DB in tensor form\n",
        "# Get id sequence of notes\n",
        "\n",
        "tensorized_subgraphs = torch.LongTensor([subgraphs[x] for x in subgraphs])\n",
        "print(max_len)\n",
        "print(len(subgraphs))\n",
        "print(tensorized_subgraphs[0,:20])\n",
        "print('Saving...')\n",
        "torch.save(tensorized_subgraphs,'subgraph_norel')\n",
        "print('Done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6vOs_8HZCiy",
        "outputId": "3da879b2-d0e1-4b72-bfd1-8e6e3b63f6c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(literal_id2label.items())[-1]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2901117, 9101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}
